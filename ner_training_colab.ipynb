{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 中文诈骗场景命名实体识别(NER)训练\n",
        "本notebook用于训练中文诈骗场景下的命名实体识别模型，用于识别诈骗文本中的关键实体。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 安装必要的库\n",
        "!pip install transformers datasets seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 导入基本库\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from transformers import BertTokenizerFast, BertForTokenClassification, Trainer, TrainingArguments\n",
        "import transformers  # 添加这个导入以支持EarlyStoppingCallback\n",
        "from datasets import Dataset, load_metric\n",
        "from sklearn.model_selection import train_test_split\n",
        "import logging\n",
        "from collections import Counter\n",
        "import re\n",
        "import json\n",
        "import random\n",
        "import io\n",
        "\n",
        "# 设置日志\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# 设置设备\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "logger.info(f'使用设备: {device}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 数据加载\n",
        "本项目使用专门针对中文诈骗场景的NER数据集，包含多种诈骗类型的文本和实体标注：\n",
        "1. 中奖诈骗：涉及MONEY, LINK等标签\n",
        "2. 冒充公检法：涉及NAME, PHONE, LOC等标签\n",
        "3. 刷单诈骗：涉及MONEY, ACCOUNT等标签\n",
        "4. 理财诈骗：涉及MONEY, PRODUCT, PERCENT等标签\n",
        "5. 冒充客服：涉及PHONE, LINK等标签\n",
        "6. 冒充领导：涉及NAME, MONEY, ACCOUNT等标签\n",
        "7. 疫情诈骗：涉及ACCOUNT, PHONE等标签\n",
        "8. 网络交友：涉及MONEY, ACCOUNT等标签"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 检测是否在Colab环境中\n",
        "IN_COLAB = 'google.colab' in str(get_ipython())\n",
        "\n",
        "if IN_COLAB:\n",
        "    # 导入Colab专用库\n",
        "    from google.colab import files\n",
        "    from google.colab import drive\n",
        "    import requests\n",
        "    print(\"欢迎使用Google Colab训练NER模型！\")\n",
        "    \n",
        "    # 挂载Google Drive\n",
        "    drive.mount('/content/drive')\n",
        "    \n",
        "    # 下载项目数据\n",
        "    !wget -q https://raw.githubusercontent.com/yourusername/fraud_bot/main/data/ner_training_data_enhanced.csv -O ner_training_data_enhanced.csv\n",
        "    \n",
        "    # 如果下载失败，提示上传\n",
        "    if not os.path.exists('ner_training_data_enhanced.csv'):\n",
        "        print(\"请上传NER训练数据CSV文件...\")\n",
        "        uploaded = files.upload()\n",
        "        for filename in uploaded.keys():\n",
        "            if filename.endswith('.csv'):\n",
        "                !cp \"{filename}\" ner_training_data_enhanced.csv\n",
        "                print(f\"已将 {filename} 复制为 ner_training_data_enhanced.csv\")\n",
        "                break\n",
        "else:\n",
        "    print(\"您不在Google Colab环境中，将使用本地数据。\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 加载数据集\n",
        "import pandas as pd\n",
        "\n",
        "# 在Colab中直接使用下载/上传的文件，在本地环境中使用相对路径\n",
        "data_path = 'ner_training_data_enhanced.csv' if IN_COLAB else 'data/ner_training_data_enhanced.csv'\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(data_path)\n",
        "    print(f\"成功加载数据集，共{len(df)}条记录\")\n",
        "except Exception as e:\n",
        "    print(f\"加载数据集失败: {e}\")\n",
        "    # 使用备用示例数据\n",
        "    print(\"使用备用示例数据...\")\n",
        "    data = [\n",
        "        {\"text\": \"我刚刚接到一个自称是北京公安局的电话，说我涉嫌洗钱，要求我转账5万元到安全账户\", \"labels\": \"O O O O O O B-ORG I-ORG I-ORG O O O O O O O O O O O O O B-MONEY I-MONEY O O O O\"},\n",
        "        {\"text\": \"有人自称是中国移动客服，说我的手机号码需要实名认证，让我点击链接并输入银行卡号\", \"labels\": \"O O O O B-ORG I-ORG I-ORG O O O O O O O O O O O O O O O O O O O O O O\"},\n",
        "        {\"text\": \"最近收到短信说我的微信账号在广州登录，需要点击链接确认\", \"labels\": \"O O O O O O O O B-APP O O B-LOC O O O O O O O O O\"},\n",
        "        {\"text\": \"有快递员说我的包裹到了，让我支付关税2000元\", \"labels\": \"O B-ROLE O O O O O O O O O O O B-MONEY I-MONEY\"},\n",
        "        {\"text\": \"网站上说投资比特币可以年化收益30%，只需要投入10000元\", \"labels\": \"B-CHANNEL O O O B-PRODUCT O O O O O B-PERCENT O O O O O B-MONEY I-MONEY\"}\n",
        "    ]\n",
        "    df = pd.DataFrame(data)\n",
        "    print(f\"已生成{len(df)}条示例数据\")\n",
        "\n",
        "# 显示数据前几行\n",
        "print(\"\\n数据预览:\")\n",
        "display(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 数据预处理\n",
        "处理数据格式并准备训练"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 检查数据格式\n",
        "print(f\"数据格式: {df.columns.tolist()}\")\n",
        "\n",
        "# 确保数据包含所需字段\n",
        "required_columns = [\"text\", \"labels\"]\n",
        "for col in required_columns:\n",
        "    if col not in df.columns:\n",
        "        raise ValueError(f\"数据集缺少必要的列: {col}\")\n",
        "\n",
        "# 数据统计\n",
        "print(f\"数据集大小: {len(df)}条记录\")\n",
        "\n",
        "# 标签统计分析\n",
        "all_labels = []\n",
        "for label_seq in df['labels']:\n",
        "    all_labels.extend(label_seq.split())\n",
        "\n",
        "label_counter = Counter(all_labels)\n",
        "print(\"标签分布:\")\n",
        "for label, count in label_counter.most_common():\n",
        "    print(f\"  {label}: {count}\")\n",
        "\n",
        "# 提取唯一标签\n",
        "unique_labels = sorted(list(set(all_labels)))\n",
        "print(f\"唯一标签: {unique_labels}\")\n",
        "\n",
        "# 创建标签映射\n",
        "label_list = unique_labels\n",
        "label2id = {label: i for i, label in enumerate(label_list)}\n",
        "id2label = {i: label for i, label in enumerate(label_list)}\n",
        "\n",
        "print(f\"标签映射:\")\n",
        "print(json.dumps(label2id, ensure_ascii=False, indent=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 划分训练集和验证集\n",
        "train_df, eval_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "print(f\"训练集: {len(train_df)}条, 验证集: {len(eval_df)}条\")\n",
        "\n",
        "# 转换为datasets格式\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "eval_dataset = Dataset.from_pandas(eval_df)\n",
        "\n",
        "# 加载分词器\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 数据预处理函数\n",
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized_inputs = tokenizer(\n",
        "        examples[\"text\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=128,\n",
        "        # 我们需要这个来正确地对齐标签\n",
        "        is_split_into_words=False,\n",
        "        return_offsets_mapping=True\n",
        "    )\n",
        "    \n",
        "    labels = []\n",
        "    \n",
        "    for i, label_seq in enumerate(examples[\"labels\"]):\n",
        "        label_ids = []\n",
        "        label_list = label_seq.split()\n",
        "        \n",
        "        # 这里假设标记和标签是一一对应的\n",
        "        word_ids = [None]  # 特殊标记[CLS]\n",
        "        \n",
        "        for word_idx in range(len(label_list)):\n",
        "            token_ids = tokenizer(examples[\"text\"][i][word_idx], \n",
        "                                 add_special_tokens=False)[\"input_ids\"]\n",
        "            # 可能会被分成多个token\n",
        "            for _ in range(len(token_ids)):\n",
        "                word_ids.append(word_idx)\n",
        "        \n",
        "        # 添加[SEP]特殊标记\n",
        "        word_ids.append(None)\n",
        "        \n",
        "        # 对齐标签\n",
        "        previous_word_idx = None\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None or word_idx >= len(label_list):\n",
        "                label_ids.append(-100)  # 特殊标记标签为-100\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(label2id[label_list[word_idx]])\n",
        "            else:\n",
        "                # 同一个词的部分，使用标签的前缀保持一致\n",
        "                # 例如B-XXX后面的部分应该是I-XXX\n",
        "                current_label = label_list[word_idx]\n",
        "                if current_label.startswith(\"B-\"):\n",
        "                    label_ids.append(label2id[f\"I-{current_label[2:]}\"])\n",
        "                else:\n",
        "                    label_ids.append(label2id[current_label])\n",
        "            previous_word_idx = word_idx\n",
        "            \n",
        "        labels.append(label_ids)\n",
        "    \n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 处理数据\n",
        "train_dataset = train_dataset.map(tokenize_and_align_labels, batched=True)\n",
        "eval_dataset = eval_dataset.map(tokenize_and_align_labels, batched=True)\n",
        "\n",
        "print(\"数据处理完成！\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 模型定义与训练\n",
        "选择合适的中文预训练模型并设置最优训练参数"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 选择预训练模型\n",
        "MODEL_CHOICES = {\n",
        "    \"bert\": \"bert-base-chinese\",            # 原始BERT中文版\n",
        "    \"roberta\": \"hfl/chinese-roberta-wwm-ext\", # 哈工大RoBERTa中文版\n",
        "    \"macbert\": \"hfl/chinese-macbert-base\"    # MacBERT中文版\n",
        "}\n",
        "\n",
        "# 选择使用哪个模型，可以更换为roberta或macbert获得更好的效果\n",
        "MODEL_NAME = \"bert\"\n",
        "PRETRAINED_MODEL = MODEL_CHOICES[MODEL_NAME]\n",
        "print(f\"使用预训练模型: {PRETRAINED_MODEL}\")\n",
        "\n",
        "# 加载分词器\n",
        "tokenizer = BertTokenizerFast.from_pretrained(PRETRAINED_MODEL)\n",
        "\n",
        "# 定义模型\n",
        "model = BertForTokenClassification.from_pretrained(\n",
        "    PRETRAINED_MODEL, \n",
        "    num_labels=len(label_list), \n",
        "    id2label=id2label, \n",
        "    label2id=label2id\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "# 训练参数 - 针对诈骗NER的优化参数\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=f\"./{MODEL_NAME}_ner_antifraud\",\n",
        "    num_train_epochs=15,            # 增加训练轮次\n",
        "    per_device_train_batch_size=16, # 对于小数据集增大batch_size\n",
        "    per_device_eval_batch_size=16,\n",
        "    evaluation_strategy='steps',    # 更频繁的评估\n",
        "    eval_steps=50,                  # 每50步评估一次\n",
        "    save_strategy='steps',\n",
        "    save_steps=50,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model='f1',     # 使用F1作为最佳模型指标\n",
        "    greater_is_better=True,\n",
        "    warmup_ratio=0.1,               # 使用比例而不是固定步数\n",
        "    weight_decay=0.01,\n",
        "    learning_rate=3e-5,             # 略微提高学习率\n",
        "    label_smoothing_factor=0.1,\n",
        "    fp16=torch.cuda.is_available(), # 如果有GPU则使用混合精度训练\n",
        "    gradient_accumulation_steps=2,  # 梯度累积\n",
        "    save_total_limit=2              # 只保存最佳的2个模型\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 改进的评估指标计算函数\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "    \n",
        "    # 去除填充和特殊标记的预测和标签\n",
        "    true_predictions = [\n",
        "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    true_labels = [\n",
        "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    \n",
        "    # 在实体级别计算评估指标\n",
        "    entity_results = calculate_entity_metrics(true_labels, true_predictions)\n",
        "    \n",
        "    # 在标签级别计算评估指标\n",
        "    token_results = calculate_token_metrics(true_labels, true_predictions)\n",
        "    \n",
        "    # 合并两种评估结果，优先使用实体级别的F1\n",
        "    results = {\n",
        "        \"precision\": entity_results[\"precision\"], \n",
        "        \"recall\": entity_results[\"recall\"], \n",
        "        \"f1\": entity_results[\"f1\"],\n",
        "        \"token_accuracy\": token_results[\"accuracy\"]\n",
        "    }\n",
        "    \n",
        "    return results\n",
        "\n",
        "# 实体级别评估函数\n",
        "def calculate_entity_metrics(true_labels, true_predictions):\n",
        "    results = {\"precision\": 0.0, \"recall\": 0.0, \"f1\": 0.0}\n",
        "    \n",
        "    # 提取实体及类型\n",
        "    def extract_entities(seq):\n",
        "        entities = []\n",
        "        entity = []\n",
        "        entity_type = None\n",
        "        \n",
        "        for i, tag in enumerate(seq):\n",
        "            if tag.startswith(\"B-\"):\n",
        "                if entity:\n",
        "                    entities.append((entity_type, tuple(entity)))\n",
        "                    entity = []\n",
        "                entity_type = tag[2:]\n",
        "                entity.append(i)\n",
        "            elif tag.startswith(\"I-\") and entity:\n",
        "                if tag[2:] == entity_type:\n",
        "                    entity.append(i)\n",
        "            elif tag == \"O\":\n",
        "                if entity:\n",
        "                    entities.append((entity_type, tuple(entity)))\n",
        "                    entity = []\n",
        "                    entity_type = None\n",
        "        \n",
        "        if entity:\n",
        "            entities.append((entity_type, tuple(entity)))\n",
        "        \n",
        "        return entities\n",
        "    \n",
        "    # 按实体类型计算\n",
        "    entity_types = set()\n",
        "    true_by_type = {}\n",
        "    pred_by_type = {}\n",
        "    correct_by_type = {}\n",
        "    \n",
        "    for true_label, true_prediction in zip(true_labels, true_predictions):\n",
        "        true_entities = extract_entities(true_label)\n",
        "        pred_entities = extract_entities(true_prediction)\n",
        "        \n",
        "        # 收集所有实体类型\n",
        "        for entity_type, _ in true_entities:\n",
        "            entity_types.add(entity_type)\n",
        "        for entity_type, _ in pred_entities:\n",
        "            entity_types.add(entity_type)\n",
        "        \n",
        "        # 计算每种类型的实体数量\n",
        "        for entity_type, entity_indices in true_entities:\n",
        "            true_by_type[entity_type] = true_by_type.get(entity_type, 0) + 1\n",
        "            \n",
        "        for entity_type, entity_indices in pred_entities:\n",
        "            pred_by_type[entity_type] = pred_by_type.get(entity_type, 0) + 1\n",
        "            \n",
        "        for entity in pred_entities:\n",
        "            if entity in true_entities:\n",
        "                entity_type = entity[0]\n",
        "                correct_by_type[entity_type] = correct_by_type.get(entity_type, 0) + 1\n",
        "    \n",
        "    # 计算总体评估指标\n",
        "    total_true = sum(true_by_type.values())\n",
        "    total_pred = sum(pred_by_type.values())\n",
        "    total_correct = sum(correct_by_type.values())\n",
        "    \n",
        "    if total_pred > 0:\n",
        "        results[\"precision\"] = total_correct / total_pred\n",
        "    if total_true > 0:\n",
        "        results[\"recall\"] = total_correct / total_true\n",
        "    if results[\"precision\"] + results[\"recall\"] > 0:\n",
        "        results[\"f1\"] = 2 * results[\"precision\"] * results[\"recall\"] / (results[\"precision\"] + results[\"recall\"])\n",
        "    \n",
        "    # 输出每种实体类型的评估指标\n",
        "    for entity_type in sorted(entity_types):\n",
        "        true_count = true_by_type.get(entity_type, 0)\n",
        "        pred_count = pred_by_type.get(entity_type, 0)\n",
        "        correct_count = correct_by_type.get(entity_type, 0)\n",
        "        \n",
        "        precision = correct_count / pred_count if pred_count > 0 else 0\n",
        "        recall = correct_count / true_count if true_count > 0 else 0\n",
        "        f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0\n",
        "        \n",
        "        results[f\"{entity_type}_precision\"] = precision\n",
        "        results[f\"{entity_type}_recall\"] = recall\n",
        "        results[f\"{entity_type}_f1\"] = f1\n",
        "    \n",
        "    return results\n",
        "\n",
        "# 标签级别评估函数\n",
        "def calculate_token_metrics(true_labels, true_predictions):\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    \n",
        "    for true_label, true_prediction in zip(true_labels, true_predictions):\n",
        "        for tl, tp in zip(true_label, true_prediction):\n",
        "            total += 1\n",
        "            if tl == tp:\n",
        "                correct += 1\n",
        "    \n",
        "    accuracy = correct / total if total > 0 else 0\n",
        "    return {\"accuracy\": accuracy}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 创建Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=tokenizer,\n",
        "    # 添加Early Stopping回调\n",
        "    callbacks=[transformers.EarlyStoppingCallback(early_stopping_patience=3)]\n",
        ")\n",
        "\n",
        "# 训练模型\n",
        "trainer.train()\n",
        "\n",
        "# 评估模型\n",
        "eval_results = trainer.evaluate()\n",
        "print(f\"评估结果: {eval_results}\")\n",
        "\n",
        "# 显示评估指标细节\n",
        "for metric, value in eval_results.items():\n",
        "    if \"_\" in metric and (\"precision\" in metric or \"recall\" in metric or \"f1\" in metric):\n",
        "        print(f\"  {metric}: {value:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 模型保存和测试\n",
        "保存训练好的模型并进行测试"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 保存模型\n",
        "output_dir = f\"./{MODEL_NAME}_ner_antifraud_final\"\n",
        "model.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "print(f\"模型保存在 {output_dir}\")\n",
        "\n",
        "# 保存标签映射和训练配置\n",
        "config = {\n",
        "    \"id2label\": id2label,\n",
        "    \"label2id\": label2id,\n",
        "    \"model_name\": MODEL_NAME,\n",
        "    \"pretrained_model\": PRETRAINED_MODEL,\n",
        "    \"max_length\": 128,\n",
        "    \"training_args\": training_args.to_dict()\n",
        "}\n",
        "\n",
        "with open(os.path.join(output_dir, \"config.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(config, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "# 可视化训练过程\n",
        "train_results = trainer.state.log_history\n",
        "eval_loss = []\n",
        "train_loss = []\n",
        "f1_scores = []\n",
        "steps = []\n",
        "\n",
        "for item in train_results:\n",
        "    if 'loss' in item and 'epoch' in item:\n",
        "        train_loss.append(item['loss'])\n",
        "        steps.append(item['step'])\n",
        "    if 'eval_loss' in item:\n",
        "        eval_loss.append(item['eval_loss'])\n",
        "        if 'eval_f1' in item:\n",
        "            f1_scores.append(item['eval_f1'])\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(steps, train_loss, 'b-', label='Training loss')\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "if f1_scores:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(range(len(f1_scores)), f1_scores, 'g-', label='F1 Score')\n",
        "    plt.xlabel('Evaluation Step')\n",
        "    plt.ylabel('F1 Score')\n",
        "    plt.title('F1 Score Evolution')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 改进的实体预测函数\n",
        "def predict_entities(text, visualize=False):\n",
        "    # 对输入文本进行分词\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    \n",
        "    # 预测\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        predictions = torch.argmax(outputs.logits, dim=2)\n",
        "    \n",
        "    # 解码预测结果\n",
        "    predicted_token_labels = []\n",
        "    for token_predictions in predictions:\n",
        "        token_labels = []\n",
        "        for prediction in token_predictions:\n",
        "            label = id2label.get(prediction.item(), \"O\")\n",
        "            token_labels.append(label)\n",
        "        predicted_token_labels.append(token_labels)\n",
        "    \n",
        "    # 跳过特殊标记[CLS]和[SEP]\n",
        "    token_labels = predicted_token_labels[0][1:-1]\n",
        "    input_tokens = tokenizer.convert_ids_to_tokens(inputs.input_ids[0])[1:-1]\n",
        "    \n",
        "    # 提取实体\n",
        "    entities = {}\n",
        "    current_entity = []\n",
        "    current_type = None\n",
        "    char_to_token = []\n",
        "    tokens_info = []\n",
        "    \n",
        "    for i, (token, label) in enumerate(zip(input_tokens, token_labels)):\n",
        "        # 重构原始文本\n",
        "        tokens_info.append((token, label))\n",
        "        \n",
        "        if label.startswith(\"B-\"):\n",
        "            if current_entity:\n",
        "                entity_text = \"\".join(current_entity).replace(\"##\", \"\")\n",
        "                if current_type not in entities:\n",
        "                    entities[current_type] = []\n",
        "                entities[current_type].append(entity_text)\n",
        "            current_entity = [token]\n",
        "            current_type = label[2:]\n",
        "        elif label.startswith(\"I-\") and current_entity and current_type == label[2:]:\n",
        "            current_entity.append(token)\n",
        "        elif label == \"O\":\n",
        "            if current_entity:\n",
        "                entity_text = \"\".join(current_entity).replace(\"##\", \"\")\n",
        "                if current_type not in entities:\n",
        "                    entities[current_type] = []\n",
        "                entities[current_type].append(entity_text)\n",
        "                current_entity = []\n",
        "                current_type = None\n",
        "    \n",
        "    # 处理最后一个实体（如果有）\n",
        "    if current_entity:\n",
        "        entity_text = \"\".join(current_entity).replace(\"##\", \"\")\n",
        "        if current_type not in entities:\n",
        "            entities[current_type] = []\n",
        "        entities[current_type].append(entity_text)\n",
        "    \n",
        "    # 可视化结果\n",
        "    if visualize:\n",
        "        from IPython.display import HTML, display\n",
        "        import pandas as pd\n",
        "        import numpy as np\n",
        "        \n",
        "        # 创建带有HTML颜色编码的文本显示\n",
        "        html = \"<h3>标注结果:</h3><p>\"\n",
        "        \n",
        "        # 颜色映射\n",
        "        color_map = {\n",
        "            \"ORG\": \"#FF9999\",     # 淡红色\n",
        "            \"MONEY\": \"#99FF99\",   # 淡绿色\n",
        "            \"LINK\": \"#9999FF\",    # 淡蓝色\n",
        "            \"PHONE\": \"#FFFF99\",   # 淡黄色\n",
        "            \"NAME\": \"#FF99FF\",    # 淡紫色\n",
        "            \"ACCOUNT\": \"#99FFFF\", # 淡青色\n",
        "            \"TIME\": \"#FFCC99\",    # 淡橙色\n",
        "            \"LOC\": \"#99CCFF\",     # 淡蓝紫色\n",
        "            \"APP\": \"#CC99FF\",     # 紫罗兰色\n",
        "            \"ROLE\": \"#FF6666\",    # 橙红色\n",
        "            \"PRODUCT\": \"#66FF66\", # 浅绿色\n",
        "            \"CHANNEL\": \"#6666FF\", # 深蓝色\n",
        "            \"PERCENT\": \"#CCCCFF\"  # 淡紫蓝色\n",
        "        }\n",
        "        \n",
        "        current_entity_type = None\n",
        "        for token, label in tokens_info:\n",
        "            token_text = token.replace('##', '')\n",
        "            \n",
        "            if label.startswith(\"B-\"):\n",
        "                entity_type = label[2:]\n",
        "                current_entity_type = entity_type\n",
        "                color = color_map.get(entity_type, \"#CCCCCC\")\n",
        "                html += f'<span style=\"background-color: {color};\">{token_text}</span>'\n",
        "            elif label.startswith(\"I-\") and current_entity_type == label[2:]:\n",
        "                color = color_map.get(current_entity_type, \"#CCCCCC\")\n",
        "                html += f'<span style=\"background-color: {color};\">{token_text}</span>'\n",
        "            else:\n",
        "                current_entity_type = None\n",
        "                html += token_text\n",
        "                \n",
        "        html += \"</p>\"\n",
        "        \n",
        "        # 显示颜色图例\n",
        "        html += \"<h3>实体类型:</h3>\"\n",
        "        html += \"<ul style=\\\"list-style-type:none;\\\">\"\n",
        "        for entity_type, color in color_map.items():\n",
        "            if entity_type in [item[2:] for item in set([label for _, label in tokens_info if label != \"O\"])]:\n",
        "                html += f'<li><span style=\"background-color: {color}; padding: 2px 5px;\">{entity_type}</span></li>'\n",
        "        html += \"</ul>\"\n",
        "        \n",
        "        display(HTML(html))\n",
        "        \n",
        "        # 显示提取的实体表格\n",
        "        if entities:\n",
        "            print(\"\\n识别出的实体:\")\n",
        "            for entity_type, entity_values in entities.items():\n",
        "                print(f\"  {entity_type}: {', '.join(entity_values)}\")\n",
        "        else:\n",
        "            print(\"\\n未识别出任何实体\")\n",
        "    \n",
        "    return entities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 测试数据\n",
        "test_samples = [\n",
        "    \"我刚收到一条短信，说我的银行卡需要注销，要求我点击链接网址www.bank-verify.com或拨打电话13800138000\",\n",
        "    \"有人自称是公安局的让我去ATM机操作转账5000元到安全账户6217001234567890\",\n",
        "    \"刚才有个自称是中国移动的客服说我的手机号需要实名认证，让我提供身份证和银行卡\",\n",
        "    \"小王告诉我最近有个投资项目，年化收益率30%，只需投1万元就能月入3000元\",\n",
        "    \"网站上说投资比特币可以年化收益35%，只需要投入10000元到账户1234567890\"\n",
        "]\n",
        "\n",
        "print(\"测试模型效果:\")\n",
        "for i, sample in enumerate(test_samples):\n",
        "    print(f\"\\n\\n示例 {i+1}: {sample}\\n\")\n",
        "    entities = predict_entities(sample, visualize=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 模型下载与部署\n",
        "导出训练好的模型并下载，供后续在应用中使用"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 创建用于生产环境的推理脚本\n",
        "inference_script = f\"\"\"\n",
        "# 中文诈骗场景命名实体识别(NER)推理脚本\n",
        "# 基于模型: {PRETRAINED_MODEL}\n",
        "# 训练时间: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "\n",
        "import torch\n",
        "from transformers import BertTokenizerFast, BertForTokenClassification\n",
        "import json\n",
        "import os\n",
        "\n",
        "# 配置参数\n",
        "MODEL_PATH = \"./model\"  # 替换为实际模型路径\n",
        "\n",
        "# 加载标签映射\n",
        "with open(os.path.join(MODEL_PATH, \"config.json\"), \"r\", encoding=\"utf-8\") as f:\n",
        "    config = json.load(f)\n",
        "    id2label = config[\"id2label\"]\n",
        "    # 将字符串键转换为整数键\n",
        "    id2label = {{int(k): v for k, v in id2label.items()}}\n",
        "\n",
        "# 加载模型和分词器\n",
        "tokenizer = BertTokenizerFast.from_pretrained(MODEL_PATH)\n",
        "model = BertForTokenClassification.from_pretrained(MODEL_PATH)\n",
        "\n",
        "# 设置设备\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "def predict_entities(text):\n",
        "    # 分词\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
        "    inputs = {{k: v.to(device) for k, v in inputs.items()}}\n",
        "    \n",
        "    # 预测\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        predictions = torch.argmax(outputs.logits, dim=2)\n",
        "    \n",
        "    # 解码预测结果\n",
        "    predicted_token_labels = []\n",
        "    for token_predictions in predictions:\n",
        "        token_labels = []\n",
        "        for prediction in token_predictions:\n",
        "            label = id2label.get(prediction.item(), \"O\")\n",
        "            token_labels.append(label)\n",
        "        predicted_token_labels.append(token_labels)\n",
        "    \n",
        "    # 提取实体\n",
        "    entities = {{}}\n",
        "    current_entity = []\n",
        "    current_type = None\n",
        "    \n",
        "    # 跳过特殊标记[CLS]和[SEP]\n",
        "    token_labels = predicted_token_labels[0][1:-1]\n",
        "    tokens = tokenizer.convert_ids_to_tokens(inputs.input_ids[0])[1:-1]\n",
        "    \n",
        "    for i, (token, label) in enumerate(zip(tokens, token_labels)):\n",
        "        if label.startswith(\"B-\"):\n",
        "            if current_entity:\n",
        "                entity_text = \"\".join(current_entity).replace(\"##\", \"\")\n",
        "                if current_type not in entities:\n",
        "                    entities[current_type] = []\n",
        "                entities[current_type].append(entity_text)\n",
        "            current_entity = [token]\n",
        "            current_type = label[2:]\n",
        "        elif label.startswith(\"I-\") and current_entity and current_type == label[2:]:\n",
        "            current_entity.append(token)\n",
        "        elif label == \"O\":\n",
        "            if current_entity:\n",
        "                entity_text = \"\".join(current_entity).replace(\"##\", \"\")\n",
        "                if current_type not in entities:\n",
        "                    entities[current_type] = []\n",
        "                entities[current_type].append(entity_text)\n",
        "                current_entity = []\n",
        "                current_type = None\n",
        "    \n",
        "    # 处理最后一个实体（如果有）\n",
        "    if current_entity:\n",
        "        entity_text = \"\".join(current_entity).replace(\"##\", \"\")\n",
        "        if current_type not in entities:\n",
        "            entities[current_type] = []\n",
        "        entities[current_type].append(entity_text)\n",
        "    \n",
        "    return entities\n",
        "\n",
        "# 使用示例\n",
        "if __name__ == \"__main__\":\n",
        "    test_text = \"我刚收到一条短信，说我的银行卡需要注销，要求我点击链接或拨打电话13800138000\"\n",
        "    result = predict_entities(test_text)\n",
        "    print(f\"输入文本: {{test_text}}\")\n",
        "    print(\"识别出的实体:\")\n",
        "    for entity_type, entity_values in result.items():\n",
        "        print(f\"  {{entity_type}}: {{', '.join(entity_values)}}\")\n",
        "\"\"\"\n",
        "\n",
        "# 保存推理脚本\n",
        "with open(os.path.join(output_dir, \"inference.py\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(inference_script)\n",
        "\n",
        "# 创建一个简单的README文件\n",
        "readme = f\"\"\"\n",
        "# 中文诈骗场景命名实体识别(NER)模型\n",
        "\n",
        "## 模型信息\n",
        "- 预训练模型: {PRETRAINED_MODEL}\n",
        "- 训练数据集: 中文诈骗场景NER数据集\n",
        "- 训练时间: {pd.Timestamp.now().strftime('%Y-%m-%d')}\n",
        "- 支持实体类型: {', '.join(sorted(set([k[2:] for k in label2id.keys() if k != 'O' and k.startswith('B-')]))}\n",
        "\n",
        "## 快速使用\n",
        "1. 安装依赖: `pip install torch transformers`\n",
        "2. 使用 `inference.py` 脚本进行推理\n",
        "\n",
        "## 示例代码\n",
        "```python\n",
        "from inference import predict_entities\n",
        "\n",
        "text = \"我刚收到一条短信，说我的银行卡需要注销，要求我点击链接或拨打电话13800138000\"\n",
        "entities = predict_entities(text)\n",
        "print(entities)\n",
        "```\n",
        "\"\"\"\n",
        "\n",
        "with open(os.path.join(output_dir, \"README.md\"), \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(readme)\n",
        "\n",
        "# Colab环境下，压缩并提供下载\n",
        "if IN_COLAB:\n",
        "    # 压缩模型和相关文件\n",
        "    !zip -r \"{MODEL_NAME}_ner_antifraud_model.zip\" \"{output_dir}/\" \n",
        "    \n",
        "    # 下载模型\n",
        "    from google.colab import files\n",
        "    files.download(f\"{MODEL_NAME}_ner_antifraud_model.zip\")\n",
        "    print(f\"模型下载链接已生成，请点击下载 {MODEL_NAME}_ner_antifraud_model.zip\")\n",
        "else:\n",
        "    print(f\"模型和相关文件已保存到 {output_dir} 目录\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 结论与后续优化\n",
        "\n",
        "### 已完成工作\n",
        "1. 基于中文诈骗场景数据集训练了命名实体识别(NER)模型\n",
        "2. 该模型可识别多种诈骗类型中的关键实体，如机构(ORG)、金额(MONEY)、地点(LOC)、电话(PHONE)等\n",
        "3. 通过优化训练参数提高了模型性能\n",
        "4. 提供了可视化的实体标注结果\n",
        "5. 生成了便于部署的推理脚本\n",
        "\n",
        "### 后续优化方向\n",
        "1. **数据扩充**：增加更多诈骗场景数据，特别是新型网络诈骗文本\n",
        "2. **模型改进**：尝试更高级的预训练模型如BERT-wwm、RoBERTa、MacBERT等\n",
        "3. **多任务学习**：将NER与诈骗意图分类结合，构建多任务模型\n",
        "4. **模型蒸馏**：将训练好的大模型知识蒸馏到小模型中，提高推理速度\n",
        "5. **规则增强**：结合规则识别某些特定格式的实体（如电话号码、银行卡号等）\n",
        "\n",
        "### 实际应用\n",
        "本模型可集成到反诈骗系统中，用于：\n",
        "1. 短信/聊天记录自动检测和分析\n",
        "2. 客服系统中的风险信息提取\n",
        "3. 反诈骗知识图谱构建的基础\n",
        "4. 诈骗套路自动识别与预警\n",
        "\n",
        "在实际部署时，建议结合其他模块（如意图分类）形成完整的反诈骗AI系统。"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
} 